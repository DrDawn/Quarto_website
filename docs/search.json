[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Quarto_website",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "assumptions.html",
    "href": "assumptions.html",
    "title": "Assumptions",
    "section": "",
    "text": "Before using the results of a traditional hypothesis test, there are assumptions that must be checked and verified."
  },
  {
    "objectID": "assumptions.html#assumptions-for-anova",
    "href": "assumptions.html#assumptions-for-anova",
    "title": "Assumptions",
    "section": "Assumptions for ANOVA",
    "text": "Assumptions for ANOVA\nTraditional ANOVA (Analysis of Variance) relies on several assumptions to provide valid and reliable results. Here are the key assumptions:\n\nIndependence: Observations within and between groups should be independent of each other. This means that the values of one observation should not be influenced by or related to the values of other observations.\nNormality: The residuals (the differences between observed and predicted values) should follow a normal distribution. While ANOVA is robust to violations of normality when sample sizes are large, it’s more sensitive with smaller sample sizes.\nHomogeneity of Variance (Homoscedasticity): The variances of the residuals should be approximately equal across all groups. This assumption is known as homoscedasticity. Violations of this assumption might affect the reliability of the F-test.\nIndependence of Errors: The errors (residuals) should be independent of each other. This assumption implies that there should be no systematic pattern in the residuals.\nRandom Sampling: The data should be collected using a random sampling method to ensure that the sample is representative of the population being studied.\nInterval or Ratio Scale Data: ANOVA assumes that the dependent variable is measured on an interval or ratio scale.\n\nWhen these assumptions are violated, the results of ANOVA may be unreliable, leading to incorrect conclusions. For instance, violations of normality might affect the accuracy of p-values, and violations of homoscedasticity might influence the validity of the F-test. However, ANOVA is known to be robust to violations of normality and equal variance assumptions, especially when sample sizes are large.\nWhen assumptions are not met, alternative approaches such as non-parametric tests or transformations of the data might be more appropriate or employing robust methods like bootstrapping can be considered to address the violations and provide more accurate results."
  },
  {
    "objectID": "assumptions.html#confirmation-of-assumptions-for-traditional-anova",
    "href": "assumptions.html#confirmation-of-assumptions-for-traditional-anova",
    "title": "Assumptions",
    "section": "Confirmation of Assumptions for traditional ANOVA",
    "text": "Confirmation of Assumptions for traditional ANOVA\nConfirming that the assumptions for ANOVA are met involves several diagnostic steps to assess the data. Here are ways to check for the assumptions:\n\nNormality:\n\nVisual Inspection: Histograms, Q-Q plots, or density plots can be used to visually assess whether the residuals (differences between observed and predicted values) approximately follow a normal distribution for each group.\nStatistical Tests: Shapiro-Wilk test, Kolmogorov-Smirnov test, or Anderson-Darling test can be employed to formally test for normality of residuals. However, these tests can be sensitive to sample size.\n\nHomogeneity of Variance:\n\nVisual Inspection: Plotting residuals against predicted values or group categories can help detect patterns or trends in variance.\nStatistical Tests: Levene’s test or Bartlett’s test can be used to formally test the homogeneity of variance assumption among groups. However, these tests can be sensitive to departures from normality.\n\nIndependence of Errors:\n\nCheck for patterns in residuals over time or across the data collection process. If there are systematic patterns, it might indicate a lack of independence.\n\nRandom Sampling:\n\nEnsure that the data was collected randomly from the population of interest. This assumption might not be directly tested but is important to consider in study design.\n\n\nTo confirm these assumptions:\n\nVisual inspection of diagnostic plots can be informative but might not be conclusive.\nStatistical tests can provide formal assessments, but they might lack robustness, especially with smaller sample sizes.\nConsideration of the context of the study and the potential impact of violations on the ANOVA results is crucial.\n\nIf assumptions are not met:\n\nTransformations of the data might be applied to meet assumptions (e.g., log transformation).\nRobust statistical methods or non-parametric tests (e.g., Kruskal-Wallis test instead of ANOVA) can be considered.\nResampling methods like bootstrapping can provide more robust estimates in the presence of violations.\n\nOverall, no single method can definitively confirm all assumptions, so a combination of approaches and careful consideration of the data’s context is essential."
  },
  {
    "objectID": "assumptions.html#assumptions-for-correlation-test",
    "href": "assumptions.html#assumptions-for-correlation-test",
    "title": "Assumptions",
    "section": "Assumptions for Correlation Test",
    "text": "Assumptions for Correlation Test\nWhen performing a traditional hypothesis test for a correlation coefficient using the cor.test() function in R or any other method, certain assumptions should be considered. These assumptions include:\n\nLinear Relationship: The test assumes that the relationship between the variables is linear. If the relationship is not linear, the correlation coefficient might not accurately represent the association between the variables.\nNormality: The variables should follow a bivariate normal distribution. While the test is robust to violations of normality for large sample sizes, for smaller sample sizes, normality is an important assumption.\nHomoscedasticity: It’s assumed that the variability of the data points around the regression line is constant (homoscedastic). If the variability changes with the value of the independent variable, it is termed heteroscedasticity, which might affect the accuracy of the correlation coefficient.\nNo Outliers: The presence of outliers can significantly affect the correlation coefficient. It’s essential to check for outliers and their impact on the correlation.\nIndependence: The data points should be independent of each other. In some cases, especially with time series data or other specific cases, observations might not be independent.\n\nViolating these assumptions can affect the accuracy and interpretation of the correlation coefficient. Therefore, it’s advisable to check for these assumptions before interpreting the results of a correlation test. Additionally, non-parametric tests (e.g., Spearman’s rank correlation) might be more appropriate if the assumptions for Pearson’s correlation are not met."
  },
  {
    "objectID": "assumptions.html#confirmation-of-assumptions-for-traditional-correlation-test",
    "href": "assumptions.html#confirmation-of-assumptions-for-traditional-correlation-test",
    "title": "Assumptions",
    "section": "Confirmation of Assumptions for traditional Correlation Test",
    "text": "Confirmation of Assumptions for traditional Correlation Test\nTo confirm whether the assumptions for conducting a correlation test are met, you can perform several diagnostic checks and statistical tests. Here are some methods to assess the assumptions:\n\nVisual Inspection:\n\nScatterplot: Plot the data using a scatterplot to visually inspect the relationship between the variables. Check for linearity and outliers.\nResidual Plot: If you perform a regression to test the correlation, examine the residual plot to assess homoscedasticity. Residuals should be evenly distributed around zero.\n\nNormality Check:\n\nNormal Q-Q plot: Check the normality assumption using a Q-Q plot. If the points fall along a diagonal line, the assumption of normality might be reasonable. You can also use statistical tests like Shapiro-Wilk or Kolmogorov-Smirnov to test for normality.\n\nTests for Homoscedasticity:\n\nBreusch-Pagan Test or White Test: These are formal tests to check for homoscedasticity in regression models. They test whether the variances of the errors/residuals are constant.\n\nOutlier Detection:\n\nBoxplot: Create a boxplot for each variable to detect outliers.\nCook’s distance: For regression analysis, Cook’s distance can help identify influential points that might affect the correlation.\n\nAssessing Independence:\n\nFor time series data, autocorrelation plots or Durbin-Watson tests can be used to detect autocorrelation.\n\n\nPerforming these diagnostic checks will help you assess whether the assumptions for a correlation test are reasonably met. If these assumptions are violated, you might need to consider alternative methods or transformations to address these issues. In cases of severe violations, using non-parametric correlation tests such as Spearman’s or Kendall’s correlation might be more appropriate, as they are less sensitive to these assumptions."
  },
  {
    "objectID": "assumptions.html#assumptions-for-a-chi-square-goodness-of-fit-test",
    "href": "assumptions.html#assumptions-for-a-chi-square-goodness-of-fit-test",
    "title": "Assumptions",
    "section": "Assumptions for a Chi-square Goodness of Fit Test",
    "text": "Assumptions for a Chi-square Goodness of Fit Test\nGoodness-of-fit tests assess how well an observed frequency distribution fits an expected theoretical distribution. The assumptions for conducting a goodness-of-fit test primarily depend on the specific test being used. However, some general assumptions are common across these tests:\nRandom Sampling: The data used in the test should be obtained through a random sampling process to ensure representativeness of the population.\nCategorical Data: The data should be categorical in nature, consisting of counts or frequencies within different categories or groups.\nIndependence: The observations should be independent of each other. Each individual or item should belong to only one category and not influence the category of another.\nLarge Sample Size (for certain tests): For some goodness-of-fit tests (like the chi-square goodness-of-fit test), larger expected frequencies in each category (usually at least 5) are assumed to ensure the validity of the asymptotic distribution of the test statistic.\nExpected Frequencies: The expected frequencies (often derived from a theoretical distribution or assumed proportions) should be known or estimable for each category or group being compared.\nMutually Exclusive Categories: Categories or groups being compared should be mutually exclusive, meaning an individual or item can belong to only one category."
  },
  {
    "objectID": "assumptions.html#confirmation-of-assumptions-methods",
    "href": "assumptions.html#confirmation-of-assumptions-methods",
    "title": "Assumptions",
    "section": "Confirmation of Assumptions Methods",
    "text": "Confirmation of Assumptions Methods\nTheoretical Model Specification: The expected frequencies should be derived from a specific theoretical model or hypothesis about the distribution of the categorical variable. The goodness-of-fit test assesses how well the observed data aligns with this theoretical model.\n\nRandom Sampling: Ensure that the data is collected through a random sampling method. This could involve examining the data collection process, understanding how samples were selected, and confirming that it was done without bias.\nCategorical Data: Verify that your data is categorical, consisting of counts or frequencies within distinct categories. Check the nature of your variables to ensure they fall into categorical types.\nIndependence: Independence can be assessed by examining the data collection methodology and ensuring that each observation is independent of others. For instance, in survey data, ensure that responses from different individuals are not influenced by each other.\nLarge Sample Size (if required): If your test assumes larger expected frequencies in each category, confirm that your sample size is sufficient to meet this assumption. This might involve assessing the expected frequencies in each category based on your sample.\nExpected Frequencies: Calculate or estimate the expected frequencies for each category based on your theoretical model or hypothesis. Ensure that these expected frequencies align with your research expectations.\nMutually Exclusive Categories: Check that each observation belongs to only one category or group. There should be no overlap or ambiguity in categorizing the data.\nTheoretical Model Specification: Evaluate the appropriateness of the chosen theoretical model. This might involve reviewing prior research, understanding the context of your data, and ensuring that the model fits the nature of your categorical variable."
  },
  {
    "objectID": "assumptions.html#assumptions-for-the-proportions-test",
    "href": "assumptions.html#assumptions-for-the-proportions-test",
    "title": "Assumptions",
    "section": "Assumptions for the Proportions Test",
    "text": "Assumptions for the Proportions Test\nThe 1-sample proportion test, often performed using functions like prop_test() in the infer package in R, is used to assess whether a sample proportion differs significantly from a hypothesized population proportion. When conducting this test, it’s important to consider several assumptions and conditions:\n\nRandom Sampling: The sample should be collected through a random process to ensure that it is representative of the population from which it’s drawn.\nIndependence: Individual observations within the sample should be independent of each other. For example, in survey data, one respondent’s answer should not influence another respondent’s answer.\nBinary Outcome: The variable being analyzed should be categorical and binary in nature, indicating two possible outcomes (e.g., success/failure, yes/no).\nSample Size: There are guidelines for the sample size based on the distribution of the sample proportion. Generally, a rule of thumb is that both the number of successes (events) and failures should be at least 5 in the sample. This guideline ensures that the sampling distribution of the sample proportion is approximately normal.\nLarge Population Assumption: For a finite population, if the sample size is more than 5% of the population, adjustments may need to be made in the calculation of the standard error to account for the finite population correction.\nTheoretical Conditions for Inference: Underlying these assumptions is the assumption that the sampling distribution of the sample proportion is approximately normal, especially for large sample sizes (due to the Central Limit Theorem). This assumption supports the use of normal approximations for significance testing and confidence intervals."
  },
  {
    "objectID": "assumptions.html#confirmation-of-assumptions-for-the-traditional-1-sample-proportion-test",
    "href": "assumptions.html#confirmation-of-assumptions-for-the-traditional-1-sample-proportion-test",
    "title": "Assumptions",
    "section": "Confirmation of Assumptions for the traditional 1-sample proportion test:",
    "text": "Confirmation of Assumptions for the traditional 1-sample proportion test:\nAssumption 1: Random Sampling\n\nCheck Data Collection Methodology: Review the method used to collect the sample. Ensure it was obtained randomly or via a process that represents the population without bias.\n\nAssumption 2: Binary Outcome\n\nExamine Data Structure: Verify that the variable being analyzed is categorical with two distinct categories (e.g., yes/no, success/failure).\n\nAssumption 3: Independence\n\nData Collection Process: Confirm that individual observations are independent and not influenced by each other. For instance, in survey data, one respondent’s answer shouldn’t influence another respondent’s answer.\nTemporal Independence: If data was collected over time, ensure observations across time points are not correlated.\n\nAssumption 4: Sample Size\n\nCount Successes and Failures: Calculate and ensure that both the number of successes and failures in the sample are reasonably large (typically at least 5 for each).\nSample Size Adequacy: Assess if the overall sample size is sufficient for the analysis.\n\nAssumption 5: Large Population Assumption (if applicable)\n\nRelative to Population Size: If dealing with a finite population, ensure the sample size is not too large relative to the population size. Adjustments might be needed in calculations to account for this.\n\nAdditional Consideration:\n\nNormality Assumption: Although not a strict assumption for the one-sample proportion test, for larger sample sizes, consider examining the distribution of the sample proportions. This can be done visually using histograms or normality tests to ensure that the sampling distribution of proportions is approximately normal due to the Central Limit Theorem.\n\n\nTools for Confirmation:\n\nDescriptive Statistics: Compute summary statistics (e.g., proportions, counts) to understand the nature of the data.\nVisualizations: Use bar plots or histograms to visualize the distribution of the categorical variable.\nReview Documentation: Understand the data collection process and any known biases or influences.\nSimulations (if needed): Utilize bootstrapping or simulation methods to assess the variability of proportions."
  },
  {
    "objectID": "assumptions.html#assumptions-for-the-test-for-independence",
    "href": "assumptions.html#assumptions-for-the-test-for-independence",
    "title": "Assumptions",
    "section": "Assumptions for the Test for Independence",
    "text": "Assumptions for the Test for Independence\nThe Chi-square Test for Independence is used to determine if there’s a significant association between two categorical variables. Several assumptions need to be met for this test to be valid:\n\nIndependence of observations: The data should be collected independently and should not be related in any way.\nRandom sampling: The data should be obtained through a random sampling method from the population of interest.\nCategorical data: The variables under consideration must be categorical, meaning they fall into categories or groups rather than being continuous.\nExpected cell frequencies: The expected frequency in each cell of the contingency table should not be too low (typically no cell should have an expected frequency less than 5). If many cells have expected frequencies below 5, the chi-square test may not be valid, and alternatives like Fisher’s exact test might be more appropriate.\n\nIf these assumptions are met, the Chi-square test can effectively analyze the association between categorical variables."
  },
  {
    "objectID": "assumptions.html#confirmation-of-the-assumptions-for-chi-square-test-for-independence",
    "href": "assumptions.html#confirmation-of-the-assumptions-for-chi-square-test-for-independence",
    "title": "Assumptions",
    "section": "Confirmation of the Assumptions for Chi-square Test for Independence",
    "text": "Confirmation of the Assumptions for Chi-square Test for Independence\nTo ensure the assumptions for the Chi-square test for independence are met, several methods can be employed:\n\nIndependence of observations:\n\nReview the data collection process to ensure that observations are indeed independent. Investigate if there are any reasons why observations might be related or dependent on each other. Random sampling:\nVerify that the data was collected using a random sampling method. If possible, check the sampling methodology used in data collection. Categorical data:\n\nConfirm that the variables being analyzed are categorical, falling into distinct groups or categories. Check the data type of variables to ensure they are categorical. Expected cell frequencies:\nCreate a contingency table with the categorical variables and review the expected frequencies in each cell. Use the formula for expected cell frequencies: Expected Frequency = ( Row Total × Column Total ) /Grand Total. ​\nEnsure that no cell has an expected frequency less than 5 (this is a guideline, not a strict rule). If there are low expected frequencies, consider combining categories or using alternative statistical methods.\nAdditionally, visual inspection of the data via graphical representations (such as bar charts or contingency tables) can sometimes provide insights into potential issues with the assumptions.\nSimpler statistical techniques like descriptive statistics may also offer preliminary insights into the nature of the data and potential violations of assumptions."
  },
  {
    "objectID": "assumptions.html#assumptions-for-t-test",
    "href": "assumptions.html#assumptions-for-t-test",
    "title": "Assumptions",
    "section": "Assumptions for T Test",
    "text": "Assumptions for T Test\nThe traditional t-test for the difference in two means assumes several conditions for its validity. The most important assumptions include:\n\nIndependence: The data points in each group should be independent of each other. One observation should not influence another.\nNormality: The data within each group should follow a normal distribution. However, the t-test is relatively robust to violations of normality, especially with larger sample sizes (Central Limit Theorem helps in this case).\nHomogeneity of Variance (Homoscedasticity): The variances within each group should be roughly equal. This assumption is important for the validity of the pooled variance formula in the t-test.\n\nIf these assumptions are reasonably met, the t-test is considered to be valid for comparing the means of two groups. It’s important to note that violation of these assumptions might affect the accuracy and reliability of the test results.\nIn cases where assumptions are not met, alternative tests or adjustments may be considered. For example, non-parametric tests like the Mann-Whitney U test (for independent samples) or Wilcoxon signed-rank test (for paired samples) could be used if normality or homogeneity of variance assumptions are not satisfied. Additionally, transformations or robust statistical methods might be applied to mitigate the impact of violated assumptions."
  },
  {
    "objectID": "assumptions.html#confirmation-of-assumptions-for-traditional-t-test",
    "href": "assumptions.html#confirmation-of-assumptions-for-traditional-t-test",
    "title": "Assumptions",
    "section": "Confirmation of Assumptions for traditional T Test",
    "text": "Confirmation of Assumptions for traditional T Test\nTo confirm whether the assumptions for conducting a t-test for the difference in two means are met, you can perform various checks and tests. Here’s a step-by-step approach to assess the assumptions:\n\nVisual Inspection:\n\nHistograms and Q-Q Plots: Check the distribution of each group using histograms and Q-Q plots to assess normality.\nBoxplots: Visualize the spread of the data in each group to look for outliers and differences in variance.\n\nStatistical Tests:\n\nNormality Tests: Utilize tests like the Shapiro-Wilk test, Kolmogorov-Smirnov test, or Anderson-Darling test to formally assess normality within each group. These tests evaluate if the data deviate significantly from a normal distribution.\nHomogeneity of Variance Test: Perform tests like Levene’s test or Bartlett’s test to check if the variances in the groups are similar. These tests determine whether the assumption of equal variances is violated.\n\nObservation of Residuals:\n\nIf you’re using a paired t-test or a t-test in the context of linear regression, examine the residuals for normality and homoscedasticity. Residuals should be normally distributed and show constant variance around zero.\n\nSample Size Consideration:\n\nLarger sample sizes tend to be more robust to violations of normality and homogeneity of variance assumptions. If the sample size is small, consider non-parametric tests that are less sensitive to these assumptions.\n\n\nIf the assumptions are not met:\n\nNon-parametric Tests: Consider using non-parametric alternatives like the Wilcoxon signed-rank test for paired samples or the Mann-Whitney U test for independent samples, which are robust to deviations from normality and variances.\nTransformation: Transform the data (e.g., logarithmic, square root) to normalize the distribution or stabilize variances.\nRobust Methods: Explore robust statistical methods that are less sensitive to assumptions, such as Welch’s t-test, which does not assume equal variances.\n\nPerforming these tests and checks helps ensure that the assumptions of the t-test are reasonably met. However, it’s essential to interpret the results cautiously if the assumptions are violated, considering alternative methods or transformations to address these issues.\nV2.1, 12/30/23\nLast Compiled r Sys.Date()"
  }
]